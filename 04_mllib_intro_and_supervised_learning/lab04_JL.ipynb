{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Lab: Supervised Learning\n",
    "### Last Updated: August 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Justin Lee\n",
    "\n",
    "### jgh2xh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "This project has two parts:\n",
    "- Part I: Classification - build and apply a logistic regression model on the Wisconsin Breast Cancer dataset.\n",
    "- Part II: Regression - build and apply a linear regression model on the California Housing dataset.\n",
    "\n",
    "**Total Possible Points: 10**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Classification (5 POINTS)\n",
    "\n",
    "Here are the specifications and grading breakdown:\n",
    "\n",
    "- the target variable is `diagnosis`\n",
    "- use `f1`, `f2` as predictors (1 PT)\n",
    "- split data into 60% training set, 40% test set \n",
    "- standardize the predictors (1 PT)\n",
    "- use seed=314 whenever a seed is needed\n",
    "- fit a Logistic Regression model with an intercept (1 PT)\n",
    "- compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/28 23:15:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "DATA_FILEPATH = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "seed = 314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wbc shape: (569, 32)\n",
      "23/09/28 23:16:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "wbc_train shape: (354, 32)\n",
      "wbc_test shape: (215, 32)\n"
     ]
    }
   ],
   "source": [
    "wbc = spark.read.csv(DATA_FILEPATH, inferSchema=True, header=True)\n",
    "\n",
    "# train-test 60-40 split\n",
    "wbc_splits = wbc.randomSplit([0.6, 0.4], seed)\n",
    "wbc_train = wbc_splits[0]\n",
    "wbc_test = wbc_splits[1]\n",
    "\n",
    "print('wbc shape:', (wbc.count(), len(wbc.columns)))\n",
    "print('wbc_train shape:', (wbc_train.count(), len(wbc_train.columns)))\n",
    "print('wbc_test shape:', (wbc_test.count(), len(wbc_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- f1: double (nullable = true)\n",
      " |-- f2: double (nullable = true)\n",
      " |-- f3: double (nullable = true)\n",
      " |-- f4: double (nullable = true)\n",
      " |-- f5: double (nullable = true)\n",
      " |-- f6: double (nullable = true)\n",
      " |-- f7: double (nullable = true)\n",
      " |-- f8: double (nullable = true)\n",
      " |-- f9: double (nullable = true)\n",
      " |-- f10: double (nullable = true)\n",
      " |-- f11: double (nullable = true)\n",
      " |-- f12: double (nullable = true)\n",
      " |-- f13: double (nullable = true)\n",
      " |-- f14: double (nullable = true)\n",
      " |-- f15: double (nullable = true)\n",
      " |-- f16: double (nullable = true)\n",
      " |-- f17: double (nullable = true)\n",
      " |-- f18: double (nullable = true)\n",
      " |-- f19: double (nullable = true)\n",
      " |-- f20: double (nullable = true)\n",
      " |-- f21: double (nullable = true)\n",
      " |-- f22: double (nullable = true)\n",
      " |-- f23: double (nullable = true)\n",
      " |-- f24: double (nullable = true)\n",
      " |-- f25: double (nullable = true)\n",
      " |-- f26: double (nullable = true)\n",
      " |-- f27: double (nullable = true)\n",
      " |-- f28: double (nullable = true)\n",
      " |-- f29: double (nullable = true)\n",
      " |-- f30: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wbc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Hyperparams\n",
    "maxIter = 100\n",
    "regParam = 0.0\n",
    "elasticNetParam = 0.0\n",
    "\n",
    "# convert diagnosis to 0 or 1\n",
    "log_reg_stage1 = StringIndexer(inputCol='diagnosis', outputCol='bin_diagnosis')\n",
    "# gather features into feature vector\n",
    "log_reg_stage2 = VectorAssembler(inputCols=['f1', 'f2'], outputCol=\"features\")\n",
    "# scale features\n",
    "log_reg_stage3 = StandardScaler(inputCol='features', outputCol='scaledFeatures')\n",
    "# logistic regression\n",
    "log_reg_stage4 = LogisticRegression(labelCol='bin_diagnosis',\n",
    "                                    featuresCol='scaledFeatures',\n",
    "                                    maxIter=maxIter, \n",
    "                                    regParam=regParam, \n",
    "                                    elasticNetParam=elasticNetParam)\n",
    "\n",
    "log_reg_pipeline = Pipeline(stages=[log_reg_stage1, log_reg_stage2, log_reg_stage3, log_reg_stage4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [3.570044628034343,1.0818379381825294]\n",
      "Intercept: -19.503257088634616\n",
      "+------------------------------------------+----------+\n",
      "|probability                               |prediction|\n",
      "+------------------------------------------+----------+\n",
      "|[0.9240174999811224,0.07598250001887763]  |0.0       |\n",
      "|[0.0014364608997718892,0.9985635391002281]|1.0       |\n",
      "|[0.8388312056356977,0.16116879436430231]  |0.0       |\n",
      "|[0.5748392470338999,0.4251607529661001]   |0.0       |\n",
      "|[0.790322784707739,0.20967721529226102]   |0.0       |\n",
      "+------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC: 0.8934253402338509\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# fit model on training set\n",
    "lr_model = log_reg_pipeline.fit(wbc_train)\n",
    "\n",
    "# make predictions on test set\n",
    "lr_preds = lr_model.transform(wbc_test)\n",
    "\n",
    "print('Coefficients: ' + str(lr_model.stages[-1].coefficients))\n",
    "print('Intercept: ' + str(lr_model.stages[-1].intercept))\n",
    "\n",
    "lr_preds.select('probability', 'prediction').show(5, False)\n",
    "\n",
    "log_reg_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                                  labelCol='bin_diagnosis',\n",
    "                                                  metricName='areaUnderROC')\n",
    "\n",
    "auroc = log_reg_evaluator.evaluate(lr_preds)\n",
    "\n",
    "print(f'Area under ROC: {auroc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Regression (5 POINTS)\n",
    "\n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Here are the specifications and grading breakdown:\n",
    "\n",
    "- Scale the response variable median_house_value, dividing by 100000 (1 PT)\n",
    "\n",
    "- Split data into train set (80%), test set (20%) using seed=314 (1 PT)\n",
    "\n",
    "- Add new predictor: `rooms_per_household`\n",
    "\n",
    "- In the training set, select all of these features and standardize them: (1 PT)\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "- Fit a linear regression model on the training set with these parameters:\n",
    "\n",
    "  - maxIter=10\n",
    "  - regParam=0.3\n",
    "  - elasticNetParam=0.8  \n",
    "\n",
    "\n",
    "- Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH2 = 'cal_housing_data_preproc_w_header.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chp shape: (20640, 11)\n",
      "chp_train shape: (12305, 11)\n",
      "chp_test shape: (8335, 11)\n"
     ]
    }
   ],
   "source": [
    "chp = spark.read.csv(DATA_FILEPATH2, inferSchema=True, header=True)\n",
    "\n",
    "# scale response variable 'median_house_value'\n",
    "chp = chp.withColumn('median_house_value_scaled', chp.median_house_value / 100000)\n",
    "\n",
    "# add new predictor 'rooms_per_household'\n",
    "chp = chp.withColumn('rooms_per_household', chp.total_rooms / chp.households)\n",
    "\n",
    "# train-test 60-40 split\n",
    "chp_splits = chp.randomSplit([0.6, 0.4], seed)\n",
    "chp_train = chp_splits[0]\n",
    "chp_test = chp_splits[1]\n",
    "\n",
    "print('chp shape:', (chp.count(), len(chp.columns)))\n",
    "print('chp_train shape:', (chp_train.count(), len(chp_train.columns)))\n",
    "print('chp_test shape:', (chp_test.count(), len(chp_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- median_house_value_scaled: double (nullable = true)\n",
      " |-- rooms_per_household: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Hyperparams\n",
    "maxIter = 10\n",
    "regParam = 0.3\n",
    "elasticNetParam = 0.8\n",
    "\n",
    "# gather features into feature vector\n",
    "feats = ['total_bedrooms', 'population', 'households', 'median_income', 'rooms_per_household']\n",
    "lin_reg_stage1 = VectorAssembler(inputCols=feats, outputCol='features')\n",
    "# scale features\n",
    "lin_reg_stage2 = StandardScaler(inputCol='features', outputCol='scaledFeatures')\n",
    "# linear regression\n",
    "lin_reg_stage3 = LinearRegression(featuresCol='scaledFeatures',\n",
    "                                  labelCol='median_house_value_scaled',\n",
    "                                  maxIter=maxIter,\n",
    "                                  regParam=regParam,\n",
    "                                  elasticNetParam=elasticNetParam)\n",
    "\n",
    "lin_reg_pipeline = Pipeline(stages=[lin_reg_stage1, lin_reg_stage2, lin_reg_stage3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.521610036433405,0.0]\n",
      "Intercept: 1.0012589806950158\n",
      "+------------------+----------------------------------------------------------------------------------------------------+\n",
      "|prediction        |scaledFeatures                                                                                      |\n",
      "+------------------+----------------------------------------------------------------------------------------------------+\n",
      "|2.1522346892941897|[0.6345808898555432,0.5585850806708528,0.5863844642006424,2.2065827499584953,1.635274572558393]     |\n",
      "|1.2185697382305414|[0.18775988875875624,0.14854093705737645,0.13812611823392912,0.41661536848758474,0.9250507609282013]|\n",
      "|1.746159743357526 |[4.142601089955849,6.079504819084838,3.7502544177098867,1.4280798117994287,3.176214083207554]       |\n",
      "|1.2365211384362043|[0.07843134593720197,0.05692586809384487,0.07036613570407708,0.4510307342815572,0.7467007180631932] |\n",
      "|1.6038935242964385|[1.2905521467848688,1.265711098399082,1.2561658299764873,1.1553354067380226,2.6094739588482443]     |\n",
      "+------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Mean Squared Error: 0.7782965449935368\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# fit model on training set\n",
    "lin_model = lin_reg_pipeline.fit(chp_train)\n",
    "\n",
    "# make predictions on test set\n",
    "lin_preds = lin_model.transform(chp_test)\n",
    "\n",
    "print('Coefficients: ' + str(lin_model.stages[-1].coefficients))\n",
    "print('Intercept: ' + str(lin_model.stages[-1].intercept))\n",
    "\n",
    "lin_preds.select('prediction', 'scaledFeatures').show(5, False)\n",
    "\n",
    "lin_reg_evaluator = RegressionEvaluator(predictionCol='prediction',\n",
    "                                        labelCol='median_house_value_scaled',\n",
    "                                        metricName='mse')\n",
    "\n",
    "mse = lin_reg_evaluator.evaluate(lin_preds)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
